#!/usr/bin/env python3
import argparse
import numpy as np
import numpy.ma as ma
import json
import multiprocessing

import os
import sys
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'lib'))
import inputparser
import common
import cluster_pairwise
import cluster_linfreq
import clustermaker
from resultserializer import Results
import util
from progressbar import progressbar

def _convert_assignment_to_clustering(Z, vids):
  uniq_Z  = set(list(Z))
  assert uniq_Z == set(range(len(uniq_Z)))
  clusters = [[vids[vidx] for vidx in np.flatnonzero(Z == cidx)] for cidx in sorted(uniq_Z)]
  return clusters

def _make_init_clusters(variants):
  vids = common.extract_vids(variants)
  clusters = [[vid] for vid in vids]
  return clusters

def _select_best(clusterings, llhs):
  best = np.argmax(llhs)
  return clusterings[best]

def _make_unique(clusterings, llhs):
  uniq_clust, idxs = np.unique(clusterings, return_index=True, axis=0)
  uniq_llhs = llhs[idxs]
  return (uniq_clust, uniq_llhs)

def _sort_clusters(clusters, variants):
  vids, V, T, T_prime, omega = inputparser.load_read_counts(variants)
  T_prime_masked = ma.masked_equal(T_prime, 0)
  assert set(vids) == set([vid for clust in clusters for vid in clust])
  vidmap = {vid: idx for idx, vid in enumerate(vids)}

  phi_hat = []
  for C in clusters:
    vidxs = [vidmap[vid] for vid in C]
    phi_hat.append(np.sum(V[vidxs], axis=0) / np.sum(T_prime_masked[vidxs], axis=0))
  phi_hat = np.array(phi_hat)

  K, S = phi_hat.shape
  phi_hat_mean = np.sum(phi_hat, axis=1) / S
  order = np.argsort(-phi_hat_mean)

  sorted_clusters = [clusters[idx] for idx in order]
  return sorted_clusters

def _make_coclust_logprior(prior, S, scale_prior_with_samps=True):
  assert 0 < prior <= 1
  logprior = {'garbage': -np.inf, 'cocluster': np.log(prior)}
  if scale_prior_with_samps:
    logprior['cocluster'] *= S
  return logprior

def _normalize_logconc(logconc, S):
  # Convert from base 10 to base e.
  logconc /= np.log10(np.e)
  logconc *= S
  logconc = np.maximum(-600, logconc)
  logconc = np.minimum(600,  logconc)
  return logconc

def _launch_parallel(run_chain, args, iter_per_chain, nchains, parallel, seed):
  import concurrent.futures
  import multiprocessing
  import queue
  import time
  import sys

  jobs = []
  total = iter_per_chain * nchains
  manager = multiprocessing.Manager()
  progress_queue = manager.Queue()

  with progressbar(total=total, desc='Clustering variants', unit='iter', dynamic_ncols=True) as pbar:
    with concurrent.futures.ProcessPoolExecutor(max_workers=parallel) as ex:
      for C in range(nchains):
        jobs.append(ex.submit(run_chain, *args, seed + C + 1, progress_queue))
      while True:
        finished = 0
        last_check = time.perf_counter()
        for J in jobs:
          if J.done():
            exception = J.exception(timeout=0.001)
            if exception is not None:
              print('Exception occurred in child process:', exception, file=sys.stderr)
              raise exception
            else:
              finished += 1

        if finished == nchains:
          break
        while time.perf_counter() - last_check < 5:
          try:
            progress_queue.get(timeout=5)
            pbar.update()
          except queue.Empty:
            pass

  results = [J.result() for J in jobs]
  return results

def _unpack_results(results):
  vids = None
  clusterings = []
  llhs = []

  for R in results:
    V, C, L = R
    if vids is None:
      vids = V
    else:
      assert V == vids
    clusterings.append(C)
    llhs.append(L)

  return (
    vids,
    np.array([row for collection in clusterings for row in collection]),
    np.array([llh for collection in llhs for llh in collection]),
  )

def _cluster(model, variants, init_clusters, logconc, coclust_prior, iterations_per_chain, seed, nchains, parallel):
  S = len(list(variants.values())[0]['var_reads'])
  logconc = _normalize_logconc(logconc, S)

  if model == 'pairwise':
    logprior = _make_coclust_logprior(coclust_prior, S)
    supervars, clustrel_posterior, _, _, _ = clustermaker.use_pre_existing(
      variants,
      logprior,
      parallel,
      init_clusters,
      [],
    )
    superclusters = clustermaker.make_superclusters(supervars)
    run_chain = cluster_pairwise.cluster
    # Using a lambda function would be cleaner than this, but lambdas can't be
    # pickled, so they don't work with multiprocessing. Apparently only
    # top-level module functions can be pickled.
    args = (variants, init_clusters, supervars, superclusters, clustrel_posterior, logconc, iterations_per_chain)
  elif model == 'linfreq':
    run_chain = cluster_linfreq.cluster
    args = (variants, init_clusters, logconc, iterations_per_chain)
  elif model == 'both':
    raise Exception('Not yet implemented')
  else:
    raise Exception('Unknown --model: %s' % model)

  if parallel > 0:
    results = _launch_parallel(run_chain, args, iterations_per_chain, nchains, parallel, seed)
  else:
    results = [run_chain(*args, seed + C + 1, progress_queue=None) for C in range(nchains)]

  vids, assigns, llhs = _unpack_results(results)
  assigns, llhs = _make_unique(assigns, llhs)
  return (vids, assigns, llhs)

def _write_full_results(vids, assigns, llhs, full_results_fn):
  # Assuming uniform prior, the softmax gets us posterior probabilities from
  # the LLH, since the prior cancels.
  post_probs = util.softmax(llhs)

  results = Results(full_results_fn)
  results.add('vids', vids)
  results.add('clusterings', assigns)
  results.add('llhs', llhs)
  results.add('post_probs', post_probs)
  results.save()

def main():
  parser = argparse.ArgumentParser(
    description='Cluster variants into subclones suitable for building clone trees',
    formatter_class=argparse.ArgumentDefaultsHelpFormatter
  )
  parser.add_argument('--seed', dest='seed', type=int,
    help='Integer seed used for pseudo-random number generator. Running with the same seed on the same inputs will produce exactly the same result.')
  parser.add_argument('--chains', dest='chains', type=int, default=1,
    help='Number of Gibbs sampling chains to run')
  parser.add_argument('--concentration', dest='logconc', type=float, default=-2,
    help='log10(alpha) for Chinese restaurant process. The larger this is, the stronger the preference for more clusters.')
  parser.add_argument('--iterations', type=int, default=1000,
    help='Number of Gibbs sampling iterations')
  parser.add_argument('--parallel', dest='parallel', type=int, default=None,
    help='Number of tasks to run in parallel. By default, this is set to the number of CPU cores on the system. On hyperthreaded systems, this will be twice the number of physical CPUs.')
  parser.add_argument('--prior', type=float, default=0.25,
    help='Pairwise coclustering prior probability. Used only for --model=pairwise or --model=both.')
  parser.add_argument('--init-from-existing', action='store_true',
    help='Initialize clusters from those already provided in `in_params_fn`')
  parser.add_argument('--model', choices=('pairwise', 'linfreq', 'both'), default='linfreq',
    help='Clustering model to use')
  parser.add_argument('--full-results',
    help='Path to file where we will write all sampled clusterings')
  parser.add_argument('ssm_fn')
  parser.add_argument('in_params_fn')
  parser.add_argument('out_params_fn')
  args = parser.parse_args()

  # NB: in my current clustering, `pairwise` inits with every variant in a
  # separate cluster, while `linfreq` inits with every variant in a single
  # cluster. I should maybe make the initializations the same, or do
  # experiments to test how sensitive results are to the initialization.

  if args.seed is not None:
    seed = args.seed
  else:
    # Maximum seed is 2**32 - 1.
    seed = np.random.randint(2**32)
  np.random.seed(seed)

  np.set_printoptions(linewidth=400, precision=3, threshold=sys.maxsize, suppress=True)
  np.seterr(divide='raise', invalid='raise', over='raise')
  parallel = args.parallel if args.parallel is not None else multiprocessing.cpu_count()

  variants = inputparser.load_ssms(args.ssm_fn)
  orig_params = inputparser.load_params(args.in_params_fn)

  garbage = orig_params.get('garbage', [])
  variants = inputparser.remove_garbage(variants, garbage)

  if args.init_from_existing:
    assert 'clusters' in orig_params
    init_clusters = orig_params['clusters']
  else:
    init_clusters = _make_init_clusters(variants)

  vids, assigns, llhs = _cluster(
    args.model,
    variants,
    init_clusters,
    args.logconc,
    args.prior,
    args.iterations,
    seed,
    args.chains,
    parallel,
  )
  if args.full_results:
    _write_full_results(vids, assigns, llhs, args.full_results)

  best_assign = _select_best(assigns, llhs)
  clusters = _convert_assignment_to_clustering(best_assign, vids)
  clusters = _sort_clusters(clusters, variants)

  # We expect `samples` will be specified.
  assert 'samples' in orig_params
  params = {
    'clusters': clusters,
    'garbage': garbage,
    'samples': orig_params['samples'],
  }
  with open(args.out_params_fn, 'w') as F:
    json.dump(params, F)

if __name__ == '__main__':
  # This is default on Unix but not macOS. Without this, resources aren't
  # inherited by subprocesses on macOS and various things break. However, on
  # macOS, it can cause crashes. See
  # https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods
  # On top of this, MacOS throws an exception since set_start_method is called elsewhere, 
  # adding the argument 'force=True' resolves the exception.
  if sys.platform == "darwin":
    multiprocessing.set_start_method('fork', force=True)
  main()
